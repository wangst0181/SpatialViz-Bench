<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>SpatialViz-Bench</title>
  <meta name="description" content="SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs" />
  <link rel="icon" href="./favicon.ico" />
  <link rel="stylesheet" href="./assets/style.css" />
  <style>
    #examples .slide{padding:0 !important;gap:6px !important;align-items:flex-start !important;display:grid !important;grid-template-columns:1fr !important;height:auto !important;min-height:0 !important}
    #examples .slide .card{padding:10px !important;height:auto !important}
    #examples .slide .card h3{margin-bottom:0px !important}
    #examples .slide .card p{margin-bottom:0px !important}
    #examples .slide img{width:95% !important;margin:0 auto !important;max-height:none !important;height:auto !important;display:block !important}
    #examples .carousel{padding:0 !important;height:auto !important}
    #examples .carousel-track{padding:0 !important;height:auto !important;align-items:flex-start !important}
    #examples .slide .img-btn{margin:0 !important;padding:0 !important}
    body{font-size:20px}
    h1{font-size:56px}
    h2{font-size:36px}
    p{font-size:20px}
    .lead{font-size:20px}
    .subtitle.strong-subtitle{font-size:32px;line-height:1.2}
    .callout{font-size:20px}
    .callout .bullets{font-size:20px}
    #news .bullets{font-size:20px}
    #visualization p{font-size:20px}
    #visualization .bullets{font-size:20px}
    #visualization .bullets li{font-size:20px}
    #visualization .subhead{font-size:28px}
    #benchmark > .container > p{font-size:20px}
    #benchmark .section-head h3{font-size:28px}
    #analysis.subhead,#results.subhead{font-size:28px}
    #analysis, #results{font-size:20px}
    #citation p{font-size:20px}
    #citation code, #citation pre{font-size:18px}
    .task-grid .card > h3{font-size:20px}
    .task-grid .card > p{font-size:18px}
    .task-item h4{font-size:20px}
    .task-item .bullets{font-size:18px}
    .table thead th{font-size:18px}
    .table td{font-size:18px}
    figcaption{font-size:18px !important}
  </style>
</head>
<body>
  <header class="topbar">
    <div class="container topbar-inner">
      <a class="brand" href="#home">SpatialViz-Bench</a>
      <nav class="nav" id="nav">
        <a href="#visualization">Spatial Visualization</a>
        <a href="#benchmark">SpatialViz-Bench</a>
        <a href="#examples">Examples</a>
        <a href="#leaderboard">Leaderboard</a>
        <a href="#results">Main Results</a>
        <a href="#analysis">Error Analysis</a>
        <a href="#citation">Citation</a>
      </nav>
      <button class="nav-toggle" aria-label="Toggle navigation" id="navToggle">‚ò∞</button>
    </div>
  </header>

  <main id="home">
    <!-- Hero -->
    <section class="hero">
      <div class="container hero-grid">
        <div class="hero-top">
          <div class="badge">Spatial Visualization ‚Ä¢ 12 Tasks ‚Ä¢ Programmatic Generation</div>
          <h1>SpatialViz-Bench</h1>
          <p class="subtitle strong-subtitle" style="font-size:28px;line-height:1.2">A Cognitively-Grounded Benchmark for Diagnosing Spatial Visualization in MLLMs</p>
        </div>
        <div class="hero-body" style="display:grid;grid-template-columns:1.2fr .8fr;gap:22px;align-items:stretch">
          <div class="hero-left">
          <p class="lead">
            SpatialViz-Bench targets the <b>core spatial visualization faculty</b>: mentally operating on images (rotate / fold / penetrate / animate)
            while <b>decoupling</b> confounding factors such as real-world scene priors, language-heavy story context, or domain knowledge.
          </p>

          <div class="cta-row">
            <a class="btn primary" href="https://arxiv.org/abs/2507.07610" target="_blank" rel="noreferrer">Paper</a>
            <a class="btn" href="https://github.com/wangst0181/SpatialViz-Bench" target="_blank" rel="noreferrer">Code</a>
            <a class="btn" href="https://huggingface.co/datasets/PLM-Team/Spatial-Visualization-Benchmark" target="_blank" rel="noreferrer">Dataset</a>
          </div>

          <div class="meta">
            <div><span class="k">Format</span><span class="v">Multiple-choice (image options)</span></div>
            <div><span class="k">Scale</span><span class="v">1,180 QA pairs (programmatically generated)</span></div>
            <div><span class="k">Abilities</span><span class="v">Rotation / Folding / Penetration / Animation</span></div>
            <div><span class="k">Goal</span><span class="v">Probe internal world model & mental simulation</span></div>
          </div>

        </div>
        <div class="hero-logo" style="padding:0;display:flex;align-items:stretch">
          <img src="./assets/images/logo_placeholder.png" alt="Logo placeholder" style="width:100%;height:100%;object-fit:cover;max-width:none" />
        </div>
        </div>
      </div>
    </section>

    <!-- News -->
    <section class="section" id="news">
      <div class="container">
        <h2>News</h2>
        <ul class="bullets">
          <li><b>[2025.05.28]:</b> We released SpatialViz-Bench, the first benchmark to evaluate spatial visualization for MLLMs.</li>
          <li><b>[2026.01.05]:</b> EASI (Holistic Evaluation of Multimodal LLMs on Spatial Intelligence) integrated SpatialViz-Bench into their open-source evaluation platform. <a href="https://github.com/EvolvingLMMs-Lab/EASI" target="_blank" rel="noreferrer" style="color:#7aa2ff;text-decoration:underline;text-underline-offset:3px">EASI on GitHub</a></li>
          <li><b>[2026.01.26]:</b> SpatialViz-Bench has been accepted as a poster at ICLR 2026. üéâ </li>
          <li><b>[2026.02.02]:</b> We released the code for generating data.</li>
        </ul>
      </div>
    </section>

    <!-- What is Spatial Visualization -->
    <section class="section" id="visualization">
      <div class="container">
        <h2>What is Spatial Visualization?</h2>
        <p class="lead">
          Spatial visualization is the ability to imagine and mentally manipulate visual images.  Many real-world problems require going from processing
          <i>visible</i> information to mentally constructing and manipulating <i>unseen</i> structures, so as to infer hidden structure, states, and dynamics
          from partial cues, a setting where current MLLMs still struggle. Crucially, models with strong <i>spatial visualization</i> skills can serve as an
          <b>efficient internal world model</b> that supports fast, lightweight internal "what-if" scenarios for predicting action outcomes, which is far more
          efficient than explicitly rendering future states with large diffusion-based video generation models.
        </p>

        <p>We establish a cognitive framework that decomposes spatial visualization tasks into two phases:</p>
        <ul class="bullets">
          <li><b>Observe</b> visible cues (<i>spatial perception</i>).</li>
          <li><b>Infer</b> unseen structure by alternating <b>mental transformation</b> (<i>spatial visualization</i>) and <b>temporary storage</b> (<i>spatial memorization</i>).</li>
        </ul>

        <figure class="wide-figure" style="width:100%">
          <button class="img-btn wide" data-img="./assets/images/fig1_overview.png" data-cap="Figure 1: Overview of SpatialViz-Bench (placeholder).">
            <img src="./assets/images/fig1_overview.png" alt="Figure 1 placeholder" style="width:95%;margin:12px auto 0;display:block;max-height:none;height:auto" />
          </button>
          <figcaption>Figure 1: Overview of SpatialViz-Bench.</figcaption>
        </figure>

        <h3 class="subhead" id="challenges" style="margin-top:40px;font-size:24px">Challenges in Evaltuating Spatial Visualization</h3>
        <div class="grid-2">
          <div class="card">
            <h3>Mis-categorization</h3>
            <p><i>Spatial visualization</i> tasks are often buried under broader categories like mathematical or logical reasoning, appearing as multimodal puzzles or 3D geometry problems. This categorization obscures the evaluation of spatial visualization as a distinct capability and focuses on "solving" a problem rather than driving research toward core spatial abilities.</p>
          </div>
          <div class="card">
            <h3>Contamination risk</h3>
            <p>Most examples are drawn from publicly available sources, online IQ tests, administrative exams, and math contests, which risks overlap between training and evaluation data and undermines reliability. The modern paradigm of pretraining on vast, scraped internet data fundamentally challenges evaluation validity, a problem exacerbated by proprietary datasets that make auditing for contamination impossible.</p>
          </div>
          <div class="card">
            <h3>Sparse & heterogeneous formats</h3>
            <p>The scarcity of items per subskill amplifies the impact of random error, reducing the precision of evaluations. Additionally, the heterogeneous formats of the tasks make it difficult to distinguish true reasoning failures from misinterpretations, complicating the assessment of spatial reasoning skills.</p>
          </div>
          <div class="card">
            <h3>Persistent difficulty</h3>
            <p>Even with potential pretraining exposure, performance remains poor. State-of-the-art systems score just 27.64 on 3D Geometry in MM-IQ and 26.00 on Descriptive Geometry in MathVision, reflecting the inherent difficulty of these tasks.</p>
          </div>
        </div>
        <p><b>Therefore, we need a standardized, dynamically updatable benchmark designed explicitly around spatial visualization.</b></p>
      </div>
    </section>

    <!-- Benchmark -->
    <section class="section" id="benchmark">
      <div class="container">
        <h2>SpatialViz-Bench</h2>

        <p class="lead">
          SpatialViz-Bench is the first benchmark designed to formally evaluate the spatial visualization capabilities of MLLMs. It is organized around 4 core sub-abilities‚Äîmental rotation, mental folding, visual penetration, and mental animation‚Äîwith 3 assessment tasks designed for each, totaling 12 tasks.  Each task includes 2 to 3 difficulty levels, with each level containing 40 or 50 test cases, comprising 1,180 question-answer pairs in total, mostly with image-based options to focus on visual reasoning.
        </p>

        <figure class="wide-figure" style="width:100%">
          <button class="img-btn wide" data-img="./assets/images/fig2_tasks.png" data-cap="Figure 2: Overview of tasks in SpatialViz-Bench (placeholder).">
            <img src="./assets/images/fig2_tasks.png" alt="Figure 2 placeholder" style="width:95%;margin:12px auto 0;display:block;max-height:none;height:auto" />
          </button>
          <figcaption>Figure 2: Overview of tasks in SpatialViz-Bench.</figcaption>
        </figure>

        <div class="task-grid" id="taskGrid"></div>

        <p>
          We build SpatialViz-Bench via <b>programmatic generation</b> (11/12 tasks) and <b>expert design</b> (1/12). For programmatic tasks, we integrate <b>Python + FreeCAD</b> to control difficulty through explicit cognitive-load parameters (e.g., number of transformation steps), while employing randomness to enhance diversity and generate distractor options with explanations for deep diagnostics.
        </p>

        <figure class="wide-figure" style="width:100%">
          <button class="img-btn wide" data-img="./assets/images/fig3_pipeline.png" data-cap="Figure 3: Programmatic generation pipeline (placeholder).">
            <img src="./assets/images/fig3_pipeline.png" alt="Figure 3 placeholder" style="width:95%;margin:12px auto 0;display:block;max-height:none;height:auto" />
          </button>
          <figcaption>Figure 3: Programmatic generation pipeline of a data instance.</figcaption>
        </figure>

        <p>
          Mechanical System is manually designed because physically consistent procedural generation is difficult. Drawing on representative public simulations as references, domain experts create each problem from scratch to probe dynamic motion propagation via visual simulation rather than caption recall or theoretical derivation.
        </p>

        <div class="section-head spaced">
          <div>
            <h3>Advantags</h3>
          </div>
        </div>
        <div class="grid-3">
          <div class="card">
            <h3>Cognitive Framework for Task Design</h3>
            <p>A hierarchical framework based on cognitive principles guides the creation of new tasks, ensuring that the tasks are well-structured and aligned with core spatial abilities.</p>
          </div>
          <div class="card">
            <h3>Unified Input Format for Consistency</h3>
            <p>A standardized input format and templates are used to reduce confounding factors, enabling fine-grained error analysis and helping to diagnose specific areas where models struggle.</p>
          </div>
          <div class="card">
            <h3>Dynamic Test Bank for Evaluation Validity</h3>
            <p>Programmatic generation and the vast pool of public simulations for expert-driven question design support a dynamically updated test bank that proactively mitigates data contamination.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Examples -->
    <section class="section" id="examples">
      <div class="container">
        <div class="section-head">
          <div>
            <h2>Examples</h2>
            <p class="lead">We present exemples of varying difficulty levels for all tasks, with each sample containing an image,  question, options, answer, and explanation.</p>
          </div>
          <div class="section-actions">
            <button class="btn small" id="exPrev">‚Üê Prev</button>
            <button class="btn small" id="exNext">Next ‚Üí</button>
          </div>
        </div>

        <div class="carousel">
          <div class="carousel-track" id="exTrack">
            <!-- Slides rendered by main.js -->
          </div>
        </div>
      </div>
    </section>

    <!-- Leaderboard -->
    <section class="section" id="leaderboard">
      <div class="container">
        <div class="section-head">
          <div>
            <h2>Leaderboard</h2>
            <p class="lead">
              Overall accuracy on SpatialViz-Bench (from paper Table 2). Use "Metric" to switch between Direct / CoT / Best.
            </p>
          </div>
          <div class="section-actions">
            <div class="pillset" role="group" aria-label="Leaderboard controls">
              <select id="lbMetric" class="select">
                <option value="best" selected>Metric: Best</option>
                <option value="direct">Metric: Direct</option>
                <option value="cot">Metric: CoT</option>
              </select>
              <select id="lbSource" class="select">
                <option value="all" selected>Source: All</option>
                <option value="closed">Source: Closed</option>
                <option value="open">Source: Open</option>
              </select>
              <input id="lbSearch" class="input" placeholder="Search model..." />
            </div>
          </div>
        </div>

        <div class="table-wrap">
          <table class="table" id="lbTable">
            <thead>
              <tr>
                <th data-sort="rank">#</th>
                <th data-sort="model">Model</th>
                <th data-sort="source">Source</th>
                <th data-sort="direct">Direct</th>
                <th data-sort="cot">CoT</th>
                <th data-sort="best">Best</th>
              </tr>
            </thead>
            <tbody id="lbBody"></tbody>
          </table>
        </div>

        <p class="muted small">
          Notes: some closed-source models are reported with a single overall entry in Table 2; missing cells show "-".
        </p>

        <h3 class="subhead" id="results">Main Results</h3>
        <figure class="wide-figure" style="width:100%">
          <button class="img-btn wide" data-img="./assets/images/fig4_results.png" data-cap="Figure 4: Overall performance with 95% CI, plus sensitivity analyses across difficulty levels at both the model and task level.">
            <img src="./assets/images/fig4_results.png" alt="Figure 4 placeholder" style="width:95%;margin:12px auto 0;display:block;max-height:none;height:auto" />
          </button>
          <figcaption>Figure 4: Overall performance with 95% CI, plus sensitivity analyses across difficulty levels at both the model and task level.</figcaption>
        </figure>
        <div class="callout">
          <ul class="bullets">
            <li>Tasks in SpatialViz-Bench are vision-dependent and reasoning-intensive.</li>
            <li>Core 3D visualization tasks reveal common model failures.</li>
            <li>Difficulty collapse is visible only in top-tier models.</li>
            <li>CoT benefits high-performing closed-source MLLMs but often degrades open-source counterparts.</li>
          </ul>
        </div>

        <h3 class="subhead" id="analysis">Error Analysis</h3>
        <figure class="wide-figure" style="width:100%">
          <button class="img-btn wide" data-img="./assets/images/fig5_error_analysis.png" data-cap="Figure 5: Error type distributions (placeholder).">
            <img src="./assets/images/fig5_error_analysis.png" alt="Figure 5 placeholder" style="width:95%;margin:12px auto 0;display:block;max-height:none;height:auto" />
          </button>
          <figcaption>Figure 5: Error type distribution comparison across representative MLLMs.</figcaption>
        </figure>
        <div class="callout">
          <ul class="bullets">
            <li>Perceptual and spatial transformation errors dominate failures</li>
            <li>Model scaling fails to resolve core spatial deficits</li>
            <li>Deficiencies found in both perception and visualization</li>
            <li>Pre-training biases drive non-simulative problem solving</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- Citation -->
    <section class="section" id="citation">
      <div class="container">
        <h2>Citation</h2>
        <p>If you use SpatialViz-Bench in your research, please cite:</p>

        <div class="codeblock">
<pre><code>@misc{wang2026spatialvizbenchcognitivelygroundedbenchmarkdiagnosing,
      title={SpatialViz-Bench: A Cognitively-Grounded Benchmark for Diagnosing Spatial Visualization in MLLMs}, 
      author={Siting Wang and Minnan Pei and Luoyang Sun and Cheng Deng and Kun Shao and Zheng Tian and Haifeng Zhang and Jun Wang},
      year={2026},
      eprint={2507.07610},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.07610}, 
}</code></pre>
        </div>

        <footer class="footer">
          <div>¬© <span id="year"></span> SpatialViz-Bench</div>
          <div class="footer-links">
            <a href="https://github.com/wangst0181/Spatial-Visualization-Benchmark" target="_blank" rel="noreferrer">GitHub</a>
            <a href="https://huggingface.co/datasets/PLM-Team/Spatial-Visualization-Benchmark" target="_blank" rel="noreferrer">Dataset</a>
            <a href="https://arxiv.org/abs/2507.07610" target="_blank" rel="noreferrer">Paper</a>
          </div>
        </footer>
      </div>
    </section>
  </main>

  <!-- Image modal -->
  <div class="modal" id="imgModal" aria-hidden="true">
    <div class="modal-backdrop" id="modalClose"></div>
    <div class="modal-card" role="dialog" aria-modal="true" aria-label="Image preview">
      <div class="modal-head">
        <div class="modal-cap" id="modalCap"></div>
        <button class="modal-x" id="modalX" aria-label="Close">‚úï</button>
      </div>
      <img id="modalImg" alt="Preview" />
    </div>
  </div>

  <script src="./assets/main.js?v=4"></script>
</body>
</html>
